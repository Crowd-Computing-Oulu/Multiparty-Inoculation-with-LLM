{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0309fbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Merged Data for Analysis ---\n",
      "    participant_number     condition  conversation_index  guessed_role\n",
      "0                    1    supportive                   1    supportive\n",
      "1                    2  refutational                   1  refutational\n",
      "2                    3    prebunking                   1    prebunking\n",
      "3                    4    supportive                   2    supportive\n",
      "4                    5  refutational                   2  refutational\n",
      "5                    7    supportive                   3  refutational\n",
      "6                    9    prebunking                   3    prebunking\n",
      "7                   10    supportive                   1  refutational\n",
      "8                   11  refutational                   1  refutational\n",
      "9                   12    prebunking                   1    prebunking\n",
      "10                  14  refutational                   2  refutational\n",
      "11                  15    prebunking                   2  refutational\n",
      "12                  16    supportive                   3  refutational\n",
      "13                  17  refutational                   3  refutational\n",
      "14                  18    prebunking                   3    prebunking\n",
      "15                  19    supportive                   1    supportive\n",
      "16                  20  refutational                   1  refutational\n",
      "17                  21    prebunking                   1    supportive\n",
      "18                  23  refutational                   2  refutational\n",
      "19                  24    prebunking                   2    prebunking\n",
      "20                  25    supportive                   3    supportive\n",
      "21                  26  refutational                   3  refutational\n",
      "22                  27    prebunking                   3  refutational\n",
      "23                  29  refutational                   1  refutational\n",
      "24                  30    prebunking                   1  refutational\n",
      "25                  31    supportive                   2  refutational\n",
      "26                  32  refutational                   2  refutational\n",
      "27                  33    prebunking                   2    prebunking\n",
      "28                  34    supportive                   3    supportive\n",
      "29                  36    prebunking                   3  refutational\n",
      "30                  37    supportive                   1  refutational\n",
      "31                  38  refutational                   1  refutational\n",
      "32                  39    prebunking                   1    prebunking\n",
      "33                  40    supportive                   2  refutational\n",
      "34                  41  refutational                   2    prebunking\n",
      "35                  42    prebunking                   2    prebunking\n",
      "36                  43    supportive                   3  refutational\n",
      "37                  44  refutational                   3  refutational\n",
      "38                  45    prebunking                   3  refutational\n",
      "39                  46    supportive                   1    supportive\n",
      "40                  47  refutational                   1  refutational\n",
      "41                  48    prebunking                   1    prebunking\n",
      "42                  49    supportive                   2    prebunking\n",
      "43                  50  refutational                   2  refutational\n",
      "44                  51    prebunking                   2    supportive\n",
      "45                  52    supportive                   3    supportive\n",
      "\\n==============================\\n\n",
      "--- Contingency Table ---\n",
      "guessed_role  prebunking  refutational  supportive\n",
      "condition                                         \n",
      "prebunking             9             5           2\n",
      "refutational           1            14           0\n",
      "supportive             1             7           7\n",
      "\\n\n",
      "Chi² = 25.364, p = 0.00004\n",
      "→ Significant difference (95% confidence that people can tell which bot is which)\n",
      "→ Significant difference (99% confidence that people can tell which bot is which)\n",
      "Observed accuracy = 0.652\n",
      "95% CI: [0.508, 0.773]\n",
      "99% CI: [0.463, 0.803]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0s/mrthspgj4j15vc9z86r673rr0000gq/T/ipykernel_56315/2672103706.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_csv_subset['guessed_role'] = df_csv_subset['guessed_role'].str.lower().str.replace(' role', '')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import io\n",
    "from scipy.stats import chi2_contingency, binomtest\n",
    "\n",
    "# --- 1. Data Loading and Preparation ---\n",
    "\n",
    "# Create an in-memory SQLite database from the image data\n",
    "db_connection = sqlite3.connect('validation-1.db')\n",
    "db_cursor = db_connection.cursor()\n",
    "\n",
    "# Load data from the database into a DataFrame\n",
    "df_db = pd.read_sql_query(\"SELECT participant_number, condition, conversation_index FROM participants\", db_connection)\n",
    "db_connection.close()\n",
    "\n",
    "\n",
    "df_csv = pd.read_csv('validation-1.csv')\n",
    "\n",
    "# --- 2. Data Merging and Cleaning ---\n",
    "\n",
    "# Rename columns for clarity and merging\n",
    "df_csv.rename(columns={\n",
    "    \"Participant Id\": \"participant_number\",\n",
    "    \"Which of the three roles did Forty portray?\": \"guessed_role\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Select only the columns needed for the analysis\n",
    "df_db_subset = df_db[['participant_number', 'condition', 'conversation_index']]\n",
    "df_csv_subset = df_csv[['participant_number', 'guessed_role']]\n",
    "\n",
    "# Standardize the 'guessed_role' data to match 'condition' data (lowercase, remove extra words)\n",
    "df_csv_subset['guessed_role'] = df_csv_subset['guessed_role'].str.lower().str.replace(' role', '')\n",
    "\n",
    "# Merge the two dataframes on the participant number\n",
    "merged_data = pd.merge(df_db_subset, df_csv_subset, on=\"participant_number\")\n",
    "\n",
    "print(\"--- Merged Data for Analysis ---\")\n",
    "print(merged_data)\n",
    "print(\"\\\\n\" + \"=\"*30 + \"\\\\n\")\n",
    "\n",
    "\n",
    "# --- 3. Statistical Analysis ---\n",
    "\n",
    "# Check if there's enough data to analyze\n",
    "if len(merged_data) < 2:\n",
    "    print(\"Insufficient matched data to perform meaningful statistical analysis.\")\n",
    "else:\n",
    "    # Build a contingency table\n",
    "    table = pd.crosstab(merged_data[\"condition\"], merged_data[\"guessed_role\"])\n",
    "    print(\"--- Contingency Table ---\")\n",
    "    print(table)\n",
    "    print(\"\\\\n\")\n",
    "\n",
    "    # Chi-square test\n",
    "    chi2, p, dof, expected = chi2_contingency(table)\n",
    "    print(f\"Chi² = {chi2:.3f}, p = {p:.5f}\")\n",
    "\n",
    "    if p < 0.05:\n",
    "        print(\"→ Significant difference (95% confidence that people can tell which bot is which)\")\n",
    "    if p < 0.01:\n",
    "        print(\"→ Significant difference (99% confidence that people can tell which bot is which)\")\n",
    "            \n",
    "    from statsmodels.stats.proportion import proportion_confint\n",
    "\n",
    "    p_hat = correct / n\n",
    "\n",
    "    # 95% confidence interval (two-sided)\n",
    "    ci_low, ci_high = proportion_confint(count=correct, nobs=n, alpha=0.05, method='wilson')\n",
    "    print(f\"Observed accuracy = {p_hat:.3f}\")\n",
    "    print(f\"95% CI: [{ci_low:.3f}, {ci_high:.3f}]\")\n",
    "\n",
    "    # 99% confidence interval\n",
    "    ci_low_99, ci_high_99 = proportion_confint(count=correct, nobs=n, alpha=0.01, method='wilson')\n",
    "    print(f\"99% CI: [{ci_low_99:.3f}, {ci_high_99:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d57f8dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Analysis for Conversation Index: 1 ---\n",
      "Contingency Table:\n",
      "guessed_role  prebunking  refutational  supportive\n",
      "condition                                         \n",
      "prebunking             4             1           1\n",
      "refutational           0             6           0\n",
      "supportive             0             2           3\n",
      "\n",
      "Chi² = 15.851, p = 0.00323\n",
      "→ Significant difference (95% confidence that people can tell which bot is which)\n",
      "→ Significant difference (99% confidence that people can tell which bot is which)\n",
      "--------------------\n",
      "Correct Guesses: 13/17\n",
      "Observed Accuracy = 0.765\n",
      "95% CI: [0.527, 0.904]\n",
      "99% CI: [0.454, 0.927]\n",
      "\n",
      "========================================\n",
      "\n",
      "--- Analysis for Conversation Index: 2 ---\n",
      "Contingency Table:\n",
      "guessed_role  prebunking  refutational  supportive\n",
      "condition                                         \n",
      "prebunking             3             1           1\n",
      "refutational           1             5           0\n",
      "supportive             1             2           1\n",
      "\n",
      "Chi² = 5.088, p = 0.27844\n",
      "--------------------\n",
      "Correct Guesses: 9/15\n",
      "Observed Accuracy = 0.600\n",
      "95% CI: [0.357, 0.802]\n",
      "99% CI: [0.296, 0.842]\n",
      "\n",
      "========================================\n",
      "\n",
      "--- Analysis for Conversation Index: 3 ---\n",
      "Contingency Table:\n",
      "guessed_role  prebunking  refutational  supportive\n",
      "condition                                         \n",
      "prebunking             2             3           0\n",
      "refutational           0             3           0\n",
      "supportive             0             3           3\n",
      "\n",
      "Chi² = 8.400, p = 0.07798\n",
      "--------------------\n",
      "Correct Guesses: 8/14\n",
      "Observed Accuracy = 0.571\n",
      "95% CI: [0.326, 0.786]\n",
      "99% CI: [0.267, 0.830]\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Run Analysis for Each Conversation Index ---\n",
    "\n",
    "# Get the unique conversation indices from the merged data\n",
    "conversation_indices = sorted(merged_data['conversation_index'].unique())\n",
    "\n",
    "for index in conversation_indices:\n",
    "    print(f\"--- Analysis for Conversation Index: {index} ---\")\n",
    "    \n",
    "    # Filter the data for the current conversation index\n",
    "    subset_data = merged_data[merged_data['conversation_index'] == index]\n",
    "    \n",
    "    # Check if there's enough data to analyze\n",
    "    if len(subset_data) < 2:\n",
    "        print(\"Insufficient matched data to perform meaningful statistical analysis for this index.\\n\")\n",
    "        continue\n",
    "\n",
    "    # --- Chi-Squared Test ---\n",
    "    # Build a contingency table\n",
    "    table = pd.crosstab(subset_data[\"condition\"], subset_data[\"guessed_role\"])\n",
    "    print(\"Contingency Table:\")\n",
    "    print(table)\n",
    "    print(\"\")\n",
    "\n",
    "    # Perform the test\n",
    "    chi2, p, dof, expected = chi2_contingency(table)\n",
    "    print(f\"Chi² = {chi2:.3f}, p = {p:.5f}\")\n",
    "\n",
    "    if p < 0.05:\n",
    "        print(\"→ Significant difference (95% confidence that people can tell which bot is which)\")\n",
    "    if p < 0.01:\n",
    "        print(\"→ Significant difference (99% confidence that people can tell which bot is which)\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    # --- Accuracy and Confidence Intervals ---\n",
    "    correct = (subset_data[\"condition\"] == subset_data[\"guessed_role\"]).sum()\n",
    "    n = len(subset_data)\n",
    "    \n",
    "    if n > 0:\n",
    "        p_hat = correct / n\n",
    "        \n",
    "        # 95% confidence interval (two-sided)\n",
    "        ci_low_95, ci_high_95 = proportion_confint(count=correct, nobs=n, alpha=0.05, method='wilson')\n",
    "        print(f\"Correct Guesses: {correct}/{n}\")\n",
    "        print(f\"Observed Accuracy = {p_hat:.3f}\")\n",
    "        print(f\"95% CI: [{ci_low_95:.3f}, {ci_high_95:.3f}]\")\n",
    "\n",
    "        # 99% confidence interval\n",
    "        ci_low_99, ci_high_99 = proportion_confint(count=correct, nobs=n, alpha=0.01, method='wilson')\n",
    "        print(f\"99% CI: [{ci_low_99:.3f}, {ci_high_99:.3f}]\")\n",
    "    else:\n",
    "        print(\"No data for accuracy analysis.\")\n",
    "        \n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
